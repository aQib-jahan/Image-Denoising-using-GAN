{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Importing Libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T08:08:58.675188Z","iopub.status.busy":"2021-10-20T08:08:58.674955Z","iopub.status.idle":"2021-10-20T08:08:58.703621Z","shell.execute_reply":"2021-10-20T08:08:58.702946Z","shell.execute_reply.started":"2021-10-20T08:08:58.675159Z"},"trusted":true},"outputs":[],"source":["import os\n","import datetime\n","import click\n","import numpy as np\n","import tqdm\n","\n","from denoise.utils import load_images, write_log\n","from denoise.losses import wasserstein_loss, perceptual_loss\n","from denoise.model import generator_model, discriminator_model, generator_containing_discriminator_multiple_outputs\n","\n","from keras.callbacks import TensorBoard\n","from tensorflow.keras.optimizers import Adam\n","\n","from numpy import load"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# This block of code is to adapt with new tensorflow verison(2.6)\n","\n","import tensorflow as tf\n","tf.config.experimental_run_functions_eagerly(True)\n","tf.config.run_functions_eagerly(True)\n","#tf.data.experimental.enable.debug_mode(True)"]},{"cell_type":"markdown","metadata":{},"source":["# **Storing Directory Settings**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T08:09:05.203898Z","iopub.status.busy":"2021-10-20T08:09:05.203348Z","iopub.status.idle":"2021-10-20T08:09:05.210945Z","shell.execute_reply":"2021-10-20T08:09:05.208843Z","shell.execute_reply.started":"2021-10-20T08:09:05.203858Z"},"trusted":true},"outputs":[],"source":["# Set the directory where the weights will be saved\n","BASE_DIR = './weights/'\n","\n","# Weight saving function\n","def save_all_weights(d, g, epoch_number, current_loss):\n","    now = datetime.datetime.now()\n","    save_dir = os.path.join(BASE_DIR, '{}{}'.format(now.month, now.day))\n","    if not os.path.exists(save_dir):\n","        os.makedirs(save_dir)\n","    g.save_weights(os.path.join(save_dir, 'generator_{}_{}.h5'.format(epoch_number, current_loss)), True)\n","    d.save_weights(os.path.join(save_dir, 'discriminator_{}.h5'.format(epoch_number)), True)"]},{"cell_type":"markdown","metadata":{},"source":["# **Parameters**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T08:09:09.494297Z","iopub.status.busy":"2021-10-20T08:09:09.493605Z","iopub.status.idle":"2021-10-20T08:09:09.498844Z","shell.execute_reply":"2021-10-20T08:09:09.498070Z","shell.execute_reply.started":"2021-10-20T08:09:09.494247Z"},"trusted":true},"outputs":[],"source":["n_images = 1500\n","batch_size = 15\n","log_dir = True\n","epoch_num = 30000\n","critic_updates=5"]},{"cell_type":"markdown","metadata":{},"source":["# **Load Data**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T08:09:25.476091Z","iopub.status.busy":"2021-10-20T08:09:25.475602Z","iopub.status.idle":"2021-10-20T08:09:53.799245Z","shell.execute_reply":"2021-10-20T08:09:53.798453Z","shell.execute_reply.started":"2021-10-20T08:09:25.476041Z"},"trusted":true},"outputs":[],"source":["data = load_images('../input/dataset/Train/', 1500)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T08:09:53.801558Z","iopub.status.busy":"2021-10-20T08:09:53.801309Z","iopub.status.idle":"2021-10-20T08:09:53.806651Z","shell.execute_reply":"2021-10-20T08:09:53.805883Z","shell.execute_reply.started":"2021-10-20T08:09:53.801523Z"},"trusted":true},"outputs":[],"source":["x_train, y_train = data['A'], data['B']"]},{"cell_type":"markdown","metadata":{},"source":["# **Train**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-20T08:10:06.887531Z","iopub.status.busy":"2021-10-20T08:10:06.887103Z","iopub.status.idle":"2021-10-20T13:30:12.786342Z","shell.execute_reply":"2021-10-20T13:30:12.784811Z","shell.execute_reply.started":"2021-10-20T08:10:06.887496Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# Load models\n","g = generator_model()\n","d = discriminator_model()\n","d_on_g = generator_containing_discriminator_multiple_outputs(g, d)\n","\n","# Set the optimizers\n","d_opt = Adam(learning_rate=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","d_on_g_opt = Adam(learning_rate=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","\n","# Set the compilers\n","d.trainable = True\n","d.compile(optimizer=d_opt, loss=wasserstein_loss)\n","d.trainable = False\n","loss = [perceptual_loss, wasserstein_loss]\n","loss_weights = [100, 1]\n","d_on_g.compile(optimizer=d_on_g_opt, loss=loss, loss_weights=loss_weights)\n","d.trainable = True\n","\n","output_true_batch, output_false_batch = np.ones((batch_size, 1)), -np.ones((batch_size, 1))\n","\n","log_path = './logs'\n","tensorboard_callback = TensorBoard(log_path)\n","\n","# Load saved weights when starting again from a trained point\n","#g.load_weights('../input/dataset/generator_120_226.h5')\n","#d.load_weights('../input/dataset/discriminator_120.h5')\n","    \n","for epoch in tqdm.tqdm(range(epoch_num)):\n","    permutated_indexes = np.random.permutation(x_train.shape[0])\n","\n","    d_losses = []\n","    d_on_g_losses = []\n","    for index in range(int(x_train.shape[0] / batch_size)):\n","        batch_indexes = permutated_indexes[index*batch_size:(index+1)*batch_size]\n","        image_blur_batch = x_train[batch_indexes]\n","        image_full_batch = y_train[batch_indexes]\n","\n","        generated_images = g.predict(x=image_blur_batch, batch_size=batch_size)\n","\n","        for _ in range(critic_updates):\n","            d_loss_real = d.train_on_batch(image_full_batch, output_true_batch)\n","            d_loss_fake = d.train_on_batch(generated_images, output_false_batch)\n","            d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n","            d_losses.append(d_loss)\n","                \n","        #print(\"d_loss: \", d_loss)\n","        #print(\"Batch :\", index+1)\n","        d.trainable = False\n","\n","        d_on_g_loss = d_on_g.train_on_batch(image_blur_batch, [image_full_batch, output_true_batch])\n","        d_on_g_losses.append(d_on_g_loss)\n","\n","        d.trainable = True\n","\n","    # write_log(tensorboard_callback, ['g_loss', 'd_on_g_loss'], [np.mean(d_losses), np.mean(d_on_g_losses)], epoch_num)\n","    print(np.mean(d_losses), np.mean(d_on_g_losses))\n","    with open('./log(1500).txt', 'a+') as f:\n","        f.write('{} - {} - {}\\n'.format(epoch, np.mean(d_losses), np.mean(d_on_g_losses)))\n","\n","    if epoch%10==0:\n","        save_all_weights(d, g, epoch, int(np.mean(d_on_g_losses)))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":4}
